{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction\n",
    "Extract spectrogram information from .vr2 files.\n",
    "\n",
    "This program should be in the same directory as the root of the data. \n",
    "\n",
    "The data should be in a folder named ```<name_of_site>``` in which the .vr2 files are located in a ```<site_name>_<data>``` folder and the output saved as .out file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that these module are installed\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal as signal\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "data_root = '.'\n",
    "site = 'marion'\n",
    "\n",
    "detrend='linear'\n",
    "NFFT=512\n",
    "noverlap=64\n",
    "scale='dB'\n",
    "scale_by_freq=False\n",
    "cmap='jet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frread(fname=None):\n",
    "    \"\"\" This is a rough translation of frread.m from J. Lichtenberger for the\n",
    "    stereo=True case, i.e. we assume orthogonal loop antenna.\n",
    "    inputs\n",
    "        fname (string): File name path to the .vr2 file to load\n",
    "    outputs\n",
    "        wh (ndarray): 2xN array with the two traces in the first and second rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # open file for reading\n",
    "    fid = open(fname, 'rb')\n",
    "    # get data from file - 16-bit signed integers\n",
    "    dat = np.fromfile(fid, dtype=np.int16)\n",
    "    # length of one frame\n",
    "    frLen = 4103  ## not sure how this is determined\n",
    "    # number of frames to read\n",
    "    nFrameRead = len(dat) / frLen\n",
    "    # data length of frame\n",
    "    adatlen = 2048\n",
    "    # length of data set\n",
    "    N = int(nFrameRead * adatlen)\n",
    "    wh = np.zeros((N, 2), dtype=float)\n",
    "    # for every frame\n",
    "    for i in np.arange(0, nFrameRead, dtype=int):\n",
    "        # indices for first component\n",
    "        i1 = np.arange(7 + i * frLen, (i + 1) * frLen, 2, dtype=int)\n",
    "        # indices for second component\n",
    "        i2 = np.arange(8 + i * frLen, (i + 1) * frLen + 0, 2, dtype=int)\n",
    "        ii = np.arange(i * adatlen, (i + 1) * adatlen, dtype=int)\n",
    "        wh[ii, 0] = dat[i1]\n",
    "        wh[ii, 1] = dat[i2]\n",
    "#     print(len(np.arange(0, nFrameRead, dtype=int)))\n",
    "    return wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vr2_to_panda(dir_name,fname, site):\n",
    "    \"\"\"Extract the data from a file a store it as a Panda DataFrame\n",
    "    inputs\n",
    "        fname    file name\n",
    "        site     name of the site where data was collected\n",
    "    outputs \n",
    "        whdf     dataframe containing the signal received by the NS and EW pointitng\n",
    "                    orthogonal loop antennas\n",
    "        fs       sampling frequency\n",
    "        t0       start time\n",
    "        t1       end time\n",
    "    \"\"\"\n",
    "    # read vr2 file\n",
    "    wh = frread(os.path.join(dir_name,fname))\n",
    "    \n",
    "    # CONSTANTS\n",
    "    # Sampling frequency (20kHz for SANAE, 40kHz for MARION )\n",
    "    fs = 2e4 if site==\"sanae\" else 4e4\n",
    "    # time step in microseconds (for dataframe index)\n",
    "    dt = 1e6 / fs\n",
    "\n",
    "    # Set the date/time format in the filename\n",
    "    # dtFormat = '%Y-%m-%dUT%H_%M_%S.%f'\n",
    "    dtFormat = '%Y-%m-%dUT%H:%M:%S.%f'\n",
    "\n",
    "    # Set up pandas dataframe\n",
    "    # Start time\n",
    "    t0 = pd.datetime.strptime(fname[0:27], dtFormat)\n",
    "    # Number of samples\n",
    "    Nsamples = len(wh[:, 0])\n",
    "    # End time\n",
    "    t1 = t0 + datetime.timedelta(0, 0, Nsamples * dt)\n",
    "    # Create index\n",
    "    tindex = pd.date_range(start=t0, periods=Nsamples, freq='50U') # freq = 50us\n",
    "\n",
    "    # Create pandas data frame from wh\n",
    "    whdf = pd.DataFrame(index=tindex, data=wh[:, 0], columns=['X'])\n",
    "    whdf['Y'] = wh[:, 1]\n",
    "    # The 'X' and 'Y' columns are the signal received by the North/South and\n",
    "    # East/West pointing orthogonal loop antennas used at Marion and SANAE\n",
    "    \n",
    "    return whdf, fs, t0, t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(data, fs):\n",
    "    \"\"\"Compute spectrogram from vr2 data collected\n",
    "    inputs\n",
    "        data       Pandas DataFrame of the vr2 data\n",
    "        fs         Sampling frequency\n",
    "    outputs\n",
    "        data_info  dictionary of the frequencies, time, and spectrum of the sprectrogram\n",
    "    \"\"\"\n",
    "#     spectrogram, frequencies, times, img = plt.specgram(data.X.values, Fs=fs, detrend=detrend, NFFT=NFFT , \n",
    "#                                                         noverlap=noverlap, scale=scale,\n",
    "#                                                         scale_by_freq=scale_by_freq, cmap=cmap)\n",
    "    frequencies, times, spectrogram = signal.spectrogram(data.X.values, fs=fs, detrend=detrend, nfft=NFFT , \n",
    "                                                        noverlap=noverlap, scaling='spectrum')\n",
    "    data_info = {\n",
    "        'frequencies':frequencies,\n",
    "        'times':times,\n",
    "        'spectrogram':spectrogram,\n",
    "    }\n",
    "    return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_root, site):\n",
    "    \"\"\"Extract all vr2 files in a dataset\n",
    "    inputs\n",
    "        data_root   location of data\n",
    "        site        site where the data was collected\n",
    "    outputs \n",
    "        dataset     dictionary containing the file name and their extracted\n",
    "                    data\n",
    "    \"\"\"\n",
    "    data_location = os.path.join(data_root, site, site+'_data')\n",
    "    data_files = os.listdir(data_location)\n",
    "    dataset = {}\n",
    "    num_file = 0\n",
    "    last_percent = None\n",
    "    for file in data_files:\n",
    "        if os.path.splitext(file)[1] == '.vr2':\n",
    "            try:\n",
    "                data, fs, t0, t1 = vr2_to_panda(data_location,file, site)\n",
    "                data_info = spectrogram(data, fs)\n",
    "                dataset[file]=data_info\n",
    "                num_file += 1\n",
    "            except Exception as e:\n",
    "                print('Error:',e) \n",
    "        # print progression\n",
    "        percent = int(num_file*100/len(data_files))\n",
    "        if last_percent != percent:\n",
    "            if percent%2==0:\n",
    "                sys.stdout.write(\"%s%%\" % percent)\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                sys.stdout.write(\".\")\n",
    "                sys.stdout.flush()\n",
    "            if percent>=98:\n",
    "                print()\n",
    "            last_percent = percent\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output(data_root, site):\n",
    "    \"\"\"Extract the output information for each file\n",
    "    inputs\n",
    "        data_root   location of the data\n",
    "        site        site where data was collected\n",
    "    outputs\n",
    "        dataset     dictionary mapping each file with the whistler location\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(data_root,site)\n",
    "    output_file = None\n",
    "    last_percent = None\n",
    "    for file in os.listdir(output_path):\n",
    "        if file.endswith('.out'):\n",
    "            output_file = file\n",
    "            break\n",
    "    try:\n",
    "        os.path.exists(output_file)\n",
    "        with open(os.path.join(output_path, output_file), 'r') as f:\n",
    "            dataset = {}\n",
    "            num_line = 0\n",
    "            lines = f.readlines()\n",
    "            file_list = []\n",
    "            for line in lines:\n",
    "                event = {}\n",
    "                line = line.split('\\n') # Remove the '\\n' character from each line\n",
    "                line = line[0].split(' ') \n",
    "                line = list(filter(None, line)) # discard empty element in array\n",
    "                for index in range(2,len(line),2): # store event and probabilities in a dictionary\n",
    "                    event[line[index]]=line[index+1]\n",
    "                # save the dictionary\n",
    "                if line[1] not in file_list: # if file name not in the list\n",
    "                    dataset[line[1]]=event\n",
    "                    file_list.append(line[1])\n",
    "                else:\n",
    "                    data = dataset[line[1]]\n",
    "                    event.update(data)\n",
    "                    dataset[line[1]]=event\n",
    "                # print progression\n",
    "                percent = int(num_line*100/len(lines))\n",
    "                num_line+=1\n",
    "                if last_percent != percent:\n",
    "                    if percent%2==0:\n",
    "                        sys.stdout.write(\"%s%%\" % percent)\n",
    "                        sys.stdout.flush()\n",
    "                    else:\n",
    "                        sys.stdout.write(\".\")\n",
    "                        sys.stdout.flush()\n",
    "                    if percent>=98:\n",
    "                        print()\n",
    "                    last_percent = percent\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data_root, site):\n",
    "    \"\"\"Extract the data from the vr2 files and the output file and\n",
    "    save it as a file\n",
    "    inputs\n",
    "        data_root   location of the data\n",
    "        site        site where data was collected\n",
    "    output\n",
    "        dataset\n",
    "    \"\"\"\n",
    "    dataset, dataset_file, dataset_file_data = {}, {}, {}\n",
    "    print('Start building dataset')\n",
    "    print('Extracting data from site ', site)\n",
    "    data = extract_data(data_root, site)\n",
    "    print(\"%s data extracted\" % site)\n",
    "    print('Extracting output from site ', site)\n",
    "    output = extract_output(data_root, site)\n",
    "    print(\"%s output extracted\" % site)\n",
    "    print('Merging datasets')\n",
    "    for file in data.keys():\n",
    "        dataset_file_data['frequencies']=data[file]['frequencies']\n",
    "        dataset_file_data['times']=data[file]['times']\n",
    "        dataset_file_data['spectrogram']=data[file]['spectrogram']\n",
    "        try:\n",
    "            dataset_file_data['output']=output[file]\n",
    "        except Exception as e:\n",
    "            print(file, output[file])\n",
    "            print(e)\n",
    "        dataset_file[file]=dataset_file_data\n",
    "    dataset['detrend']=detrend\n",
    "    dataset['nfft']=NFFT\n",
    "    dataset['noverlap']=noverlap\n",
    "    dataset['data']=dataset_file\n",
    "    print('Merge completed')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(data_root,site, dataset, extension='.dat'):\n",
    "    \"\"\"Save dataset to a binary file\n",
    "    inputs\n",
    "        data_root   location of the data\n",
    "        site        site where data was collected\n",
    "        dataset     dataset \n",
    "    \"\"\"\n",
    "    dataset_file = os.path.join(data_root, site+extension)\n",
    "    try:\n",
    "        f = open(dataset_file, 'wb')\n",
    "        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        print('Dataset saved to', dataset_file)\n",
    "        print('Dataset size: ',os.stat(dataset_file).st_size)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "        \n",
    "def load_dataset(data_root, site, extension='.dat'):\n",
    "    \"\"\"load dataset \n",
    "    inputs\n",
    "        data_root   location of the data\n",
    "        site        site where data was collected\n",
    "    outputs\n",
    "        dataset\n",
    "    \"\"\"\n",
    "    dataset_file = os.path.join(data_root, site+extension) \n",
    "    if os.path.exists(dataset_file):\n",
    "        try: \n",
    "            with open(dataset_file, 'rb') as f:\n",
    "                dataset = pickle.load(f)\n",
    "                print('Loaded %s in dataset' % dataset_file)\n",
    "        except Exception as e:\n",
    "            print('Error: ', e)\n",
    "    else:\n",
    "        print('Unable to find ', dataset_file)\n",
    "        dataset = None\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = build_dataset(data_root, site)\n",
    "# save_dataset(data_root, site, dataset)\n",
    "# dataset = load_dataset(data_root, site)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_dataset(data_root, site, dataset ,save=False):\n",
    "    \"\"\"\"\"\"\n",
    "    new_dataset = {}\n",
    "    keys = np.asarray(list(dataset['data'].keys()))\n",
    "    np.random.shuffle(keys)\n",
    "    for key in keys:\n",
    "        d[key] = dataset['data'][key]\n",
    "    if save:\n",
    "        save(data_root, site, new_dataset)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "data_root = '.'\n",
    "site = 'sanae'\n",
    "\n",
    "# load all data file\n",
    "data_location = os.path.join(data_root,site)\n",
    "data_capture = os.listdir(os.path.join(data_root,site,site+'_data'))\n",
    "\n",
    "data, sample, t0, t1 = vr2_to_panda(os.path.join(data_location, site+'_data'), \n",
    "                                    data_capture[np.random.randint(len(data_capture))], \n",
    "                                    site)\n",
    "data_info = spectrogram(data, sample)\n",
    "spectrum = data_info['spectrogram']\n",
    "frequencies = data_info['frequencies']\n",
    "times = data_info['times']\n",
    "print(spectrum.shape, frequencies.shape, times.shape)\n",
    "# plt.pcolormesh(times, frequencies, np.log10(spectrum), cmap=plt.get_cmap(cmap))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
