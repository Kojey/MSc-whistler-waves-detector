{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal as signal\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database full path\n",
    "database_name = 'whistlers.h5'\n",
    "database_location = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'Data')\n",
    "database_path = os.path.join(database_location,database_name)\n",
    "\n",
    "# data variables\n",
    "awd_events = 2\n",
    "sites = ['marion', 'sanae']\n",
    "\n",
    "detrend='linear'\n",
    "nfft=512\n",
    "noverlap=64\n",
    "scaling='spectrum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output(data_root, awd_event, site):\n",
    "    \"\"\"Extract the output information for each file\n",
    "    inputs\n",
    "        data_root   location of the data\n",
    "        site        site where data was collected\n",
    "    outputs\n",
    "        dataset     dictionary mapping each file with the whistler location\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(data_root,site)\n",
    "    output_file = None\n",
    "    for file in os.listdir(output_path):\n",
    "        if file.endswith('.out'):\n",
    "            output_file = file\n",
    "            break\n",
    "    try:\n",
    "        os.path.exists(output_file)\n",
    "        with open(os.path.join(output_path, output_file), 'r') as f:\n",
    "            dataset = {}\n",
    "            num_line = 0\n",
    "            lines = f.readlines()\n",
    "            file_list = []\n",
    "            last_percent = None\n",
    "            print('\\nGenerating outputs for %s/%s' %('awdEvent'+str(awd_event),site))\n",
    "            for line in lines:\n",
    "                event = {}\n",
    "                line = line.split('\\n') # Remove the '\\n' character from each line\n",
    "                line = line[0].split(' ') \n",
    "                line = list(filter(None, line)) # discard empty element in array\n",
    "                for index in range(2,len(line),2): # store event and probabilities in a dictionary\n",
    "                    event[line[index]]=line[index+1]\n",
    "                # save the dictionary\n",
    "                if line[1] not in file_list: # if file name not in the list\n",
    "                    dataset[line[1]]=event\n",
    "                    file_list.append(line[1])\n",
    "                else:\n",
    "                    data = dataset[line[1]]\n",
    "                    event.update(data)\n",
    "                    dataset[line[1]]=event\n",
    "                # print progression\n",
    "                percent = int(num_line*100/len(lines))\n",
    "                if last_percent != percent:\n",
    "                    if percent%5==0:\n",
    "                        sys.stdout.write(\"%s%%\" % percent)\n",
    "                        sys.stdout.flush()\n",
    "                    else:\n",
    "                        sys.stdout.write(\".\")\n",
    "                        sys.stdout.flush()\n",
    "                    last_percent = percent\n",
    "                num_line+=1\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "    return dataset\n",
    "\n",
    "def datetime_to_unit(datatime):\n",
    "    times = datatime.split('UT')\n",
    "    h, m, ss = times[-1].split(':')\n",
    "    s, u = ss.split('.')\n",
    "    return [h,m,s,u]\n",
    "\n",
    "def datetime_to_ms(datetime):\n",
    "    datetime = datetime_to_unit(datetime)\n",
    "    datetime[0] = float(datetime[0])*60*60*1000\n",
    "    datetime[1] = float(datetime[1])*60*1000\n",
    "    datetime[2] = float(datetime[2])*1000\n",
    "    datetime[3] = float(datetime[3])/10**(len(datetime[3]))\n",
    "    return sum(datetime)\n",
    "\n",
    "def datetime_diff(datetime2, datetime1):\n",
    "    return datetime_to_ms(datetime2)-datetime_to_ms(datetime1)\n",
    "\n",
    "def frread(fname=None):\n",
    "    \"\"\" This is a rough translation of frread.m from J. Lichtenberger for the\n",
    "    stereo=True case, i.e. we assume orthogonal loop antenna.\n",
    "    inputs\n",
    "        fname (string): File name path to the .vr2 file to load\n",
    "    outputs\n",
    "        wh (ndarray): 2xN array with the two traces in the first and second rows.\n",
    "    \"\"\"\n",
    "    # open file for reading\n",
    "    fid = open(fname, 'rb')\n",
    "    # get data from file - 16-bit signed integers\n",
    "    dat = np.fromfile(fid, dtype=np.int16)\n",
    "    # length of one frame\n",
    "    frLen = 4103  ## not sure how this is determined\n",
    "    # number of frames to read\n",
    "    nFrameRead = len(dat) / frLen\n",
    "    # data length of frame\n",
    "    adatlen = 2048\n",
    "    # length of data set\n",
    "    N = int(nFrameRead * adatlen)\n",
    "    wh = np.zeros((N, 2), dtype=float)\n",
    "    # for every frame\n",
    "    for i in np.arange(0, nFrameRead, dtype=int):\n",
    "        # indices for first component\n",
    "        i1 = np.arange(7 + i * frLen, (i + 1) * frLen, 2, dtype=int)\n",
    "        # indices for second component\n",
    "        i2 = np.arange(8 + i * frLen, (i + 1) * frLen + 0, 2, dtype=int)\n",
    "        ii = np.arange(i * adatlen, (i + 1) * adatlen, dtype=int)\n",
    "        wh[ii, 0] = dat[i1]\n",
    "        wh[ii, 1] = dat[i2]\n",
    "#     print(len(np.arange(0, nFrameRead, dtype=int)))\n",
    "    return wh\n",
    "\n",
    "def vr2_to_panda(dir_name,fname, site):\n",
    "    \"\"\"Extract the data from a file a store it as a Panda DataFrame\n",
    "    inputs\n",
    "        fname    file name\n",
    "        site     name of the site where data was collected\n",
    "    outputs \n",
    "        whdf     dataframe containing the signal received by the NS and EW pointitng\n",
    "                    orthogonal loop antennas\n",
    "        fs       sampling frequency\n",
    "        t0       start time\n",
    "        t1       end time\n",
    "    \"\"\"\n",
    "    # read vr2 file\n",
    "    wh = frread(os.path.join(dir_name,fname))\n",
    "    \n",
    "    # CONSTANTS\n",
    "    # Sampling frequency (20kHz for SANAE, 40kHz for MARION )\n",
    "    fs = 2e4 if site==\"sanae\" else 4e4\n",
    "    # time step in microseconds (for dataframe index)\n",
    "    dt = 1e6 / fs\n",
    "\n",
    "    # Set the date/time format in the filename\n",
    "    # dtFormat = '%Y-%m-%dUT%H_%M_%S.%f'\n",
    "    dtFormat = '%Y-%m-%dUT%H:%M:%S.%f'\n",
    "\n",
    "    # Set up pandas dataframe\n",
    "    # Start time\n",
    "    t0 = pd.datetime.strptime(fname[0:27], dtFormat)\n",
    "    # Number of samples\n",
    "    Nsamples = len(wh[:, 0])\n",
    "    # End time\n",
    "    t1 = t0 + datetime.timedelta(0, 0, Nsamples * dt)\n",
    "    # Create index\n",
    "    tindex = pd.date_range(start=t0, periods=Nsamples, freq='50U') # freq = 50us\n",
    "\n",
    "    # Create pandas data frame from wh\n",
    "    whdf = pd.DataFrame(index=tindex, data=wh[:, 0], columns=['X'])\n",
    "    whdf['Y'] = wh[:, 1]\n",
    "    # The 'X' and 'Y' columns are the signal received by the North/South and\n",
    "    # East/West pointing orthogonal loop antennas used at Marion and SANAE\n",
    "    \n",
    "    return whdf, fs\n",
    "\n",
    "def spectrogram_gen(data, fs):\n",
    "    \"\"\"Compute spectrogram from vr2 data collected\n",
    "    inputs\n",
    "        data       Pandas DataFrame of the vr2 data\n",
    "        fs         Sampling frequency\n",
    "    outputs\n",
    "        data_info  dictionary of the frequencies, time, and spectrum of the sprectrogram\n",
    "    \"\"\"\n",
    "    frequencies, times, spectrogram = signal.spectrogram(data.X.values, fs=fs, detrend=detrend, nfft=nfft, \n",
    "                                                        noverlap=noverlap, scaling=scaling)\n",
    "    return frequencies, times, np.log10(spectrogram)\n",
    "\n",
    "def reshape_spectrogram(f, t, s):\n",
    "    f = np.asarray(f)\n",
    "    t = np.asarray(t)\n",
    "    s = np.asarray(s)\n",
    "    _t = np.concatenate(([0],t))\n",
    "    _s = np.concatenate((f[np.newaxis].T,s), axis=1)\n",
    "    sft = np.vstack((_t,_s))\n",
    "    return sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating outputs for awdEvent1/marion\n",
      "0%.........10%.........20%.........30%.........40%.........50%.........60%.........70%.........80%.........90%.........\n",
      "Generating outputs for awdEvent1/sanae\n",
      "0%.........10%.........20%.........30%.........40%.........50%.........60%.........70%.........80%.........90%.........\n",
      "Generating whistler traces for awdEvent1/marion\n",
      "0%.........10%.........20%.........30%.........40%.........50%.........60%.........70%.........80%.........90%.........\n",
      "Generating whistler traces for awdEvent1/sanae\n",
      "0%.........10%.........20%.........30%.........40%.........50%.........60%.........70%.........80%.........90%........."
     ]
    }
   ],
   "source": [
    "# dictionary that contains all the file-outputs pairs\n",
    "output_dict = {}\n",
    "# extract the outputs\n",
    "for awd_event in range(1,awd_events):\n",
    "    for site in sites:\n",
    "        # find output file\n",
    "        output_path = os.path.join(database_location, 'awdEvents'+str(awd_event), site)\n",
    "        output_file = None\n",
    "        for file in os.listdir(output_path):\n",
    "            if file.endswith('_no_duplicate.out'):\n",
    "                output_file = file\n",
    "                break        \n",
    "        # extract output\n",
    "        try:\n",
    "            os.path.exists(output_file)\n",
    "            with open(os.path.join(output_path, output_file), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                file_list = []\n",
    "                last_percent = None\n",
    "                num_file = 0\n",
    "                print('\\nGenerating outputs for %s/%s' %('awdEvent'+str(awd_event),site))\n",
    "                for line in lines:\n",
    "                    line = line.split('\\n') # Remove the '\\n' character from each line\n",
    "                    line = line[0].split(' ')\n",
    "                    line = list(filter(None, line)) # discard empty element in array\n",
    "                    file = line[1]\n",
    "                    file_time = file[:27]\n",
    "                    output = []\n",
    "                    for index in range(2,len(line),2): # store event and probabilities in a dictionary\n",
    "                        output.append([round(datetime_diff(line[index],file_time),5),float(line[index+1])])\n",
    "                    # sort output based on time\n",
    "                    output = sorted(output, key=lambda x:x[0])\n",
    "                    output = np.asarray(output)\n",
    "                    # save the dictionary\n",
    "                    output_dict[file] = output\n",
    "                    percent = int(num_file*100/len(lines))\n",
    "                    if last_percent != percent:\n",
    "                        if percent%10==0:\n",
    "                            sys.stdout.write(\"%s%%\" % percent)\n",
    "                            sys.stdout.flush()\n",
    "                        else:\n",
    "                            sys.stdout.write(\".\")\n",
    "                            sys.stdout.flush()\n",
    "                        last_percent = percent\n",
    "                    num_file+=1   \n",
    "        except Exception as e:\n",
    "            print('Error:', e) \n",
    "            \n",
    "spectrogram_dict = {}\n",
    "# generate specrtogram and save to dataset\n",
    "for awd_event in range(1,awd_events):\n",
    "    for site in sites:\n",
    "        f = h5py.File(database_path, 'r+')\n",
    "        f.require_group('awdEvents%s/%s/spectrograms' % (str(awd_event), site))  \n",
    "        data_location = os.path.join(database_location, 'awdEvents'+str(awd_event), site, site+'_data')\n",
    "        if os.path.exists(data_location):\n",
    "            files = [ file for file in os.listdir(data_location) if file.endswith('.vr2')] # only select .vr2 file\n",
    "            print('\\nGenerating whistler traces for %s/%s' %('awdEvent'+str(awd_event),site))\n",
    "            last_percent = None\n",
    "            num_file = 0\n",
    "            for file in files:\n",
    "                whdf, fs =  vr2_to_panda(data_location, file, site)\n",
    "#                 print(whdf.shape, fs)\n",
    "                frequencies, times, spectrogram = spectrogram_gen(whdf, fs)\n",
    "#                 print(frequencies.shape, times.shape, spectrogram.shape)\n",
    "                spec_data = reshape_spectrogram(frequencies, times, spectrogram)\n",
    "                # load group\n",
    "                grp = f[os.path.join('awdEvents'+str(awd_event),site,'spectrograms')]\n",
    "                # add group attribute\n",
    "                grp.attrs['detrend']=detrend\n",
    "                grp.attrs['nfft'] = nfft\n",
    "                grp.attrs['noverlap'] = noverlap\n",
    "                grp.attrs['scaling'] = scaling\n",
    "                # format freq, time, spec into one\n",
    "                file_dataset = grp.create_dataset(file,spec_data.shape,np.float32, compression=\"gzip\", data=spec_data)\n",
    "                file_dataset.attrs['fs'] = fs\n",
    "                try:\n",
    "                    file_dataset.attrs['output'] = output_dict[file]\n",
    "                except Exception as e:\n",
    "                    print('Could not file %s in ouput' %file)\n",
    "                percent = int(num_file*100/len(files))\n",
    "                if last_percent != percent:\n",
    "                    if percent%10==0:\n",
    "                        sys.stdout.write(\"%s%%\" % percent)\n",
    "                        sys.stdout.flush()\n",
    "                    else:\n",
    "                        sys.stdout.write(\".\")\n",
    "                        sys.stdout.flush()\n",
    "                    last_percent = percent\n",
    "                num_file+=1\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
