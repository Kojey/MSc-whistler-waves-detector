{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "sys.path.insert(0,'../../../')\n",
    "sys.path.insert(0,'../../../py')\n",
    "\n",
    "import parameters\n",
    "import utilities\n",
    "import spectrogram_utilities\n",
    "import output_utilities\n",
    "import spectrogram_output_visualiser\n",
    "import spectrogram_cuts_db_creation\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# matplotlib.get_backend()\n",
    "%matplotlib inline\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "awd_event = 1\n",
    "site = parameters.sites[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating whistler and noise cuts database for awdEvent1/marion\n",
      "0%.........10%.........20%.........30%.........40%.........50%.........60%.........70%.........80%.........90%.........\n",
      "Runtime: 252.38 seconds\n"
     ]
    }
   ],
   "source": [
    "files = utilities.all_files(awd_event, site)\n",
    "spectrogram_cuts_db_creation.spectrogram_cuts_h5py(awd_event, site,verbose=True)\n",
    "# spectrogram_cuts_db_creation.spectrogram_cuts_tfrecords(awd_event, site,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SHUFFLE = 100\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 5\n",
    "path = os.path.join(parameters.tfrecord_location,'awdEvents'+str(awd_event),'cuts')\n",
    "files = tf.data.Dataset.list_files(os.path.join(path,'*.tfr'))\n",
    "print(type(files))\n",
    "dataset = tf.data.TFRecordDataset(files, num_parallel_reads=8)\n",
    "\n",
    "dataset = dataset.shuffle(NUM_SHUFFLE)\n",
    "dataset = dataset.repeat(NUM_EPOCHS)\n",
    "features = {\n",
    "    'data': tf.FixedLenFeature([], tf.float32),\n",
    "    'merit': tf.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "dataset = dataset.map(lambda x: tf.parse_single_example(x, features))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert features.shape[0]==labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(data,[-1, f_cut_length, t_cut_length, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    '''Model function for CNN.'''\n",
    "    \n",
    "    # input layer\n",
    "    input_layer = tf.reshape(features,[-1, f_cut_length, t_cut_length, 1])\n",
    "    \n",
    "    # convolutional layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "                inputs=input_layer,\n",
    "                filters=8,\n",
    "                kernel_size=[5,5],\n",
    "                padding='same',\n",
    "                activation=tf.nn.relu)\n",
    "    \n",
    "    # pooling layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], strides=2)\n",
    "    \n",
    "    # convolutional layer #2 and pooling layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "                inputs=pool1,\n",
    "                filters=16,\n",
    "                kernel_size=[5,5],\n",
    "                padding='same',\n",
    "                activaltion=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2)\n",
    "    \n",
    "    # denser layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7*7*64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    # logit layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2)\n",
    "    \n",
    "    predictions = {\n",
    "        # generate predictions for (PREDICT and EVAL mode)\n",
    "        'classes': tf.argmax(input=logits, axis=1),\n",
    "        # add 'softmax_tensor' to the graph. It is used for PREDICT and by the 'looging_hook'\n",
    "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    if mode==tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    # calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    # configure the training optimizer (for TRAIN mode)\n",
    "    if mode==tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    # add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions['classes'])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = preprocessing.scale(data)\n",
    "\n",
    "start = time.time()\n",
    "end = time.time()\n",
    "print(\"\\nRuntime: {:.2f} seconds\".format(end - start))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
