{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import tqdm\n",
    "import time \n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "# from keras.models import load_model\n",
    "\n",
    "# import python library\n",
    "sys.path.append(os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'py'))\n",
    "\n",
    "from cfar_detector import CFARDetector\n",
    "from database import Database\n",
    "from dataset_simulation import DatasetSimulation\n",
    "\n",
    "class CFARDetectorGenerator(Database,DatasetSimulation):\n",
    "    # Attributes\n",
    "    __train, __test = None, None\n",
    "    __train_test_file = 'train_test.pickle'\n",
    "    __dataset_location = None\n",
    "    __result_location = None\n",
    "    __site = None\n",
    "    # Initializer\n",
    "    def __init__(self, dataset_location, database_location, dataset_sim_location, result_location, site,t_res, f_res):\n",
    "        self.__dataset_location = dataset_location\n",
    "        self.__result_location = result_location\n",
    "        self.__site = site\n",
    "        Database.__init__(self,dataset_location, database_location, site)\n",
    "        DatasetSimulation.__init__(self,dataset_sim_location, t_res, f_res)\n",
    "        \n",
    "    ###############################\n",
    "    \"\"\"CROSS-CORRELATION RESULTS\"\"\"\n",
    "    \n",
    "    def get_sample_corr(self, params):\n",
    "        \"\"\"Get result of cross-correlation and target indices\n",
    "        Param\n",
    "            params: parameters passed by multiprocessing correlation generator\n",
    "        Result\n",
    "            file: name of file sample\n",
    "            corr: result of correlation\n",
    "            noise_ix: index of noise cut\n",
    "            target_ix: index of target cut\"\"\"\n",
    "        sample = params[0]\n",
    "        corr = sample.get_corr(transforms=params[1], transforms_params=params[2], kernel=params[3])\n",
    "        cuts, w, _ = sample.cuts(cut_time=0.2, cut_time_split=1.5, cut_freq=8, cut_freq_min=1.5, time_err=1, noise=True)\n",
    "        noise_ix = np.array([[c[2],c[3]] for c in cuts[w:]])\n",
    "        target_ix = np.array([[c[2],c[3]] for c in cuts[:w]])\n",
    "        target_ix = target_ix[np.where(target_ix[:,1]<len(corr))]\n",
    "        return [sample.get_file(), corr, noise_ix, target_ix]\n",
    "    \n",
    "    def generate_correlation(self, transforms, transforms_params, kernel_type='sim', train=True, save=True, n=None):\n",
    "        \"\"\"Generate all correlation results\n",
    "        Param\n",
    "            save: to save results to a file\n",
    "            n: number of sample to process, None to process all sample\n",
    "        Return\n",
    "            results\n",
    "        \"\"\"\n",
    "        # get files from either train or test\n",
    "        files = self.get_train() if train else self.get_test()\n",
    "        samples = np.array([CFARDetector(self.__dataset_location, self.__site, file) for file in files])#[int(len(files)*0.297):int(len(files)*0.299)]\n",
    "        assert len(samples)>0, 'No samples'\n",
    "        # load kernel\n",
    "        kernel_mean_params = [os.path.join(self.get_database_location(), self.get_site(), 'kernels', 'kernel_data.h5')]\n",
    "        kernel_sim_params = [0.35,80,1]\n",
    "        kernel = samples[0].load_kernel(kernel_type,kernel_mean_params if kernel_type=='mean' else kernel_sim_params)\n",
    "        # create multiprocessing methods\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        params = [[sample, transforms, transforms_params, kernel] for sample in samples][:n]\n",
    "        results = []\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(self.get_sample_corr, params), total=len(params)):\n",
    "            results.append(result)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        if save:\n",
    "            self.save_generated_correlation(transforms, transforms_params, kernel_type,train,results)\n",
    "        return results\n",
    "    \n",
    "    def save_generated_correlation(self, transforms, transforms_params, kernel_type,train,results):\n",
    "        \"\"\"Save results\"\"\"\n",
    "        #create parameters dictionary\n",
    "        data = {\n",
    "            'transforms': transforms,\n",
    "            'transforms_params': transforms_params, \n",
    "            'kernel_type': kernel_type, \n",
    "            'train': train, \n",
    "            'results': results\n",
    "        }\n",
    "        path = os.path.join(self.__result_location,self.get_site(),'cfar')\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        file_name = '_'.join([str(transforms),str(transforms_params),str(kernel_type),str(train)])\n",
    "        file_name += '.result'\n",
    "        pickle.dump(data, open(os.path.join(path,file_name), 'wb'))\n",
    "    \n",
    "    def load_generated_correlation(self,transforms, transforms_params, kernel_type, train):\n",
    "        \"\"\"Load saved result containing all correlation\"\"\"\n",
    "        file_name = '_'.join([str(transforms),str(transforms_params),str(kernel_type),str(train)])\n",
    "        file_name += '.result'\n",
    "        path = os.path.join(self.__result_location,self.get_site(),'cfar',file_name)\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception('%s does not exists.'%path)\n",
    "        return pickle.load( open(path, \"rb\"))\n",
    "    \n",
    "    def load_generated_correlation_interference(self,transforms, transforms_params, kernel_type, train):\n",
    "        \"\"\"Load interference and target+interference of correlations\n",
    "        Param\n",
    "            ...\n",
    "        Return\n",
    "            target_data: contain target+interference of all correlation\n",
    "            noise_data: contain interference of all correlation\n",
    "        \"\"\"\n",
    "        results = self.load_generated_correlation(transforms, transforms_params, kernel_type, train)['results']\n",
    "        target_data, noise_data = np.array([]), np.array([])\n",
    "        for result in tqdm.tqdm(results):\n",
    "            _, corr, noise_ix, target_ix = result\n",
    "            for t in target_ix:\n",
    "                target_data = np.concatenate((target_data,corr[t[0]:t[1]]))\n",
    "            for n in noise_ix:\n",
    "                noise_data = np.concatenate((noise_data,corr[n[0]:n[1]]))  \n",
    "        return target_data, noise_data\n",
    "    \n",
    "    ###################################\n",
    "    \"\"\"CROSS-CORRELATION GENERATIONS\"\"\"\n",
    "    \n",
    "    def get_confusion_matrix_(self, params):\n",
    "        return self.get_confusion_matrix(file=params[0],trans=params[1],trans_params=params[2],kernel=params[3],\n",
    "                                         N=params[4],G=params[5],k=params[6],Ts=params[7],Tl=params[8],pfa=params[9],time_err=params[10])\n",
    "    \n",
    "    def get_confusion_matrix(self, file, trans, trans_params,kernel,N,G,k,Ts,Tl,pfa,time_err):\n",
    "        sample = CFARDetector(Database.get_dataset_location(self), Database.get_site(self), file)\n",
    "#         output = sample.detection_starting_locations_final(trans,trans_params, kernel, 'fusion_cfar', [N,G,k,Ts,Tl,pfa], time_err)\n",
    "        bboxes = sample.detection_bounding_boxes(trans,trans_params, kernel, 'fusion_cfar', [N,G,k,Ts,Tl,pfa], time_err=time_err, duration=False)\n",
    "        confusion_matrix = sample.confusion_matrix(bboxes,time_err=time_err)\n",
    "        return file, confusion_matrix, bboxes\n",
    "    \n",
    "    def dataset_cross_correlation_gen(self, transforms, transforms_params, f_min, f_max, An, D0, magnitude,N,G,k,Ts,Tl,X_dB, train=True, n=None, time_err=1, force=False):\n",
    "        \"\"\"Generate cross-correlation of all sample and save it to a file\n",
    "        Param\n",
    "            \"\"\"\n",
    "        # check if file already exist\n",
    "        t_name = '('+','.join([str(transforms),str(transforms_params)])+')'\n",
    "        f_name = '('+','.join([str(f_min),str(f_max)])+')'\n",
    "        k_name = '('+','.join([str(An),str(D0),str(magnitude)])+')'\n",
    "        d_name = '('+','.join([str(N),str(G),str(k),str(Ts),str(Tl),str(X_dB)])+')'\n",
    "        p_name = '('+','.join([str(train),str(n),str(time_err)])+')'\n",
    "        file_name = '_'.join([t_name,f_name,k_name,d_name,p_name])\n",
    "        file_name += '.corr'\n",
    "        path = os.path.join(self.__result_location,self.get_site(), 'cfar')\n",
    "        if os.path.exists(os.path.join(path,file_name)) and not force:\n",
    "            print('%s already exists'%file_name)\n",
    "            return\n",
    "        # calculate pfa\n",
    "        pfa = (1/(1+((10**(X_dB/10))/(2*N))))**(2*N)\n",
    "        # generate kernel\n",
    "        trans = ['slice',transforms,'scale']\n",
    "        trans_params = [[f_min,f_max],transforms_params,[0,1]]\n",
    "        f_range = np.linspace(f_min, f_max, 1e3)*1e3\n",
    "        dataset_sim = DatasetSimulation\n",
    "        dataset_sim.set_frequency_range(self,f_range)\n",
    "        kernel = dataset_sim.whistler_sim(self,An=An, D0=D0, magnitude=magnitude)\n",
    "        # get samples\n",
    "        files = Database.get_train(self) if train else Database.get_test(self)\n",
    "        # get results\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        params = [[file,trans,trans_params,kernel,N,G,k,Ts,Tl,pfa,time_err] for file in files][:n]\n",
    "        results = {}\n",
    "        bboxes = {}\n",
    "        confusion_matrix = [0,0,0,0]\n",
    "        for file, result, bbox in tqdm.tqdm(pool.imap_unordered(self.get_confusion_matrix_, params), total=len(params)):\n",
    "            for i in range(len(result)):\n",
    "                confusion_matrix[i]+=len(result[i])\n",
    "            results[str(file)]=result\n",
    "            bboxes[str(file)]=bbox\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \"\"\"Save results\"\"\"\n",
    "        #create parameters dictionary\n",
    "        data = {\n",
    "            'transforms': transforms, 'transforms_params': transforms_params, \n",
    "            'f_min':f_min, 'f_max':f_max,\n",
    "            'An':An, 'D0':D0, 'magnitude':magnitude,\n",
    "            'N':N, 'G':G, 'k':k,'Ts':Ts, 'Tl':Tl, 'X_dB':X_dB,\n",
    "            'train': train, 'n':n, 'time error':time_err,\n",
    "            'confusion matrix': confusion_matrix,\n",
    "            'bboxes':bboxes,\n",
    "            'results': results\n",
    "        }\n",
    "        path = os.path.join(self.__result_location,self.get_site(), 'cfar')\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        t_name = '('+','.join([str(transforms),str(transforms_params)])+')'\n",
    "        f_name = '('+','.join([str(f_min),str(f_max)])+')'\n",
    "        k_name = '('+','.join([str(An),str(D0),str(magnitude)])+')'\n",
    "        d_name = '('+','.join([str(N),str(G),str(k),str(Ts),str(Tl),str(X_dB)])+')'\n",
    "        p_name = '('+','.join([str(train),str(n),str(time_err)])+')'\n",
    "        file_name = '_'.join([t_name,f_name,k_name,d_name,p_name])\n",
    "        file_name += '.corr'\n",
    "        pickle.dump(data, open(os.path.join(path,file_name), 'wb'))\n",
    "    \n",
    "    def dataset_cross_correlation_load(self, transforms, transforms_params, f_min, f_max, An, D0, magnitude,N,G,k,Ts,Tl,X_dB, train=True, n=None, time_err=1):\n",
    "        \"\"\"Load saved result containing all correlation\"\"\"\n",
    "        t_name = '('+','.join([str(transforms),str(transforms_params)])+')'\n",
    "        f_name = '('+','.join([str(f_min),str(f_max)])+')'\n",
    "        k_name = '('+','.join([str(An),str(D0),str(magnitude)])+')'\n",
    "        d_name = '('+','.join([str(N),str(G),str(k),str(Ts),str(Tl),str(X_dB)])+')'\n",
    "        p_name = '('+','.join([str(train),str(n),str(time_err)])+')'\n",
    "        file_name = '_'.join([t_name,f_name,k_name,d_name,p_name])\n",
    "        file_name += '.corr'\n",
    "        path = os.path.join(self.__result_location,self.get_site(), 'cfar',file_name)\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception('%s does not exists.'%path)\n",
    "        return pickle.load( open(path, \"rb\"))\n",
    "\n",
    "    def performance(self, confusion_matrix):\n",
    "        tp,fp,fn,tn = confusion_matrix\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1_score = 2*(precision*recall/(precision+recall))\n",
    "        g_measure = sqrt((tp/(tp+fp))*(tp/(tp+fn)))\n",
    "        result = {\n",
    "            'recall': np.round(recall,3),\n",
    "            'precision': np.round(precision,3),\n",
    "            'f1 score': np.round(f1_score,3),\n",
    "            'g measure': np.round(g_measure,3),\n",
    "            'false alarm': np.round(1-precision,3),\n",
    "            'misdetection': np.round(1-recall,3)\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 0, 0]\n",
      "{'recall': 1.0, 'precision': 0.8, 'f1 score': 0.889, 'g measure': 0.894, 'false alarm': 0.2, 'misdetection': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','datasets', 'awdEvents1')\n",
    "# database_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','databases', 'awdEvents1')\n",
    "# dataset_sim_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','datasets', 'simulations', 'simple' ,'whistler')\n",
    "# result_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','results', 'awdEvents1')\n",
    "# site = 'marion'\n",
    "# t_res, f_res = 0.006395061728395062, 0.15503875968992248\n",
    "# cfar_detector_gen = CFARDetectorGenerator(dataset_loc, database_loc, dataset_sim_loc, result_loc, site, t_res, f_res)\n",
    "# n=1\n",
    "# cfar_detector_gen.dataset_cross_correlation_gen(transforms='zscore', transforms_params=None, f_min=1.5, f_max=9.5, \n",
    "#                                            An=0.35, D0=80, magnitude=1, N=10, G=7, k=9, Ts=3, Tl=8, X_dB=0.5,\n",
    "#                                            train=True, n=n)\n",
    "# data = cfar_detector_gen.dataset_cross_correlation_load(transforms='zscore', transforms_params=None, f_min=1.5, f_max=9.5, \n",
    "#                                            An=0.35, D0=80, magnitude=1, N=10, G=7, k=9, Ts=3, Tl=8, X_dB=0.5,\n",
    "#                                            train=True, n=n)\n",
    "# confusion_matrix = data['confusion matrix']\n",
    "# print(confusion_matrix)\n",
    "# performance = cfar_detector_gen.performance(confusion_matrix)\n",
    "# print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
