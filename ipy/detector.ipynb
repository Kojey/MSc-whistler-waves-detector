{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import tqdm\n",
    "import time \n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from keras.models import load_model\n",
    "\n",
    "# import python library\n",
    "sys.path.append(os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'py'))\n",
    "\n",
    "from sample_detector import SampleDetector\n",
    "from database import Database\n",
    "\n",
    "class Detector(Database):\n",
    "    # Attributes\n",
    "    __train, __test = None, None\n",
    "    __train_test_file = 'train_test.pickle'\n",
    "    __dataset_location = None\n",
    "    __result_location = None\n",
    "    __site = None\n",
    "    # Initializer\n",
    "    def __init__(self, dataset_location, database_location, result_location, site):\n",
    "        self.__dataset_location = dataset_location\n",
    "        self.__result_location = result_location\n",
    "        self.__site = site\n",
    "        super().__init__(dataset_location, database_location, site)\n",
    "        \n",
    "####### MATCHED FILTER\n",
    "    def evaluate_detector(self,args):\n",
    "        sample = args[0]\n",
    "        return sample.evaluate_detector(transforms=args[1], transforms_params=args[2], \n",
    "                                        detector=args[3], detector_params=args[4],\n",
    "                                        diff_err=args[5], time_err=args[6],\n",
    "                                        kernel=args[7],\n",
    "                                        segmented=args[8])\n",
    "\n",
    "    \n",
    "    def generate_kernel(self, sample, whistler, whistler_params):\n",
    "        if whistler=='sim':\n",
    "            return sample.whistler_sim(decay=whistler_params[0], \n",
    "                                       whistler_time=whistler_params[1], \n",
    "                                       whistler_freq_len=whistler_params[2], \n",
    "                                       whistler_freq_start=whistler_params[3], \n",
    "                                       thickness=whistler_params[4],\n",
    "                                       size=whistler_params[5],\n",
    "                                       freq_slice=whistler_params[6])\n",
    "        elif whistler=='mean':\n",
    "            pass\n",
    "    \n",
    "    def detector_metric(self, train, transforms, transforms_params, detector, detector_params, diff_err, time_err,\n",
    "                         whistler, whistler_params, segmented, save=False):\n",
    "        # get files from either train or test\n",
    "        files = self.get_train() if train else self.get_test()\n",
    "        samples = np.array([SampleDetector(self.__dataset_location, self.__site, file) for file in files])#[int(len(files)*0.297):int(len(files)*0.299)]\n",
    "        assert len(samples)>0, 'No samples'\n",
    "        # generate kernel\n",
    "        kernel = self.generate_kernel(samples[0],whistler, whistler_params)\n",
    "        # create multiprocessing methods\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        params = [[sample, transforms, transforms_params, detector, detector_params, diff_err, time_err, kernel, segmented] for sample in samples]\n",
    "        results = []\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(self.evaluate_detector, params), total=len(params)):\n",
    "            results.append(result)\n",
    "        results = np.array(results)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        if save:\n",
    "            self.save_detector_metric(train, transforms, transforms_params, detector, detector_params, diff_err, time_err,\n",
    "                         whistler, whistler_params, segmented, results)\n",
    "        return results\n",
    "    \n",
    "    def save_detector_metric(self, train, transforms, transforms_params, detector, detector_params, diff_err, time_err,\n",
    "                         whistler, whistler_params, segmented, results):\n",
    "        #create parameters dictionary\n",
    "        data = {\n",
    "            'transforms': transforms,\n",
    "            'transforms_params': transforms_params, \n",
    "            'detector': detector, \n",
    "            'detector_params': detector_params,\n",
    "            'whistler': whistler,\n",
    "            'whistler_params': whistler_params, \n",
    "            'diff_err': diff_err, \n",
    "            'time_err': time_err,\n",
    "            'segmented': segmented,\n",
    "            'results': results\n",
    "        }\n",
    "        path = os.path.join(self.__result_location,self.get_site())\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        file_name = '_'.join([str(train),str(transforms),str(transforms_params),str(detector),\n",
    "                     str(detector_params),str(diff_err),str(time_err),str(whistler),str(whistler_params),str(segmented)])\n",
    "        file_name += '.result'\n",
    "        pickle.dump(data, open(os.path.join(path,file_name), 'wb'))\n",
    "    \n",
    "    def load_detector_metric(self,train, transforms, transforms_params, detector, detector_params, diff_err, time_err,\n",
    "                         whistler, whistler_params,segmented):\n",
    "        file_name = '_'.join([str(train),str(transforms),str(transforms_params),str(detector),\n",
    "                     str(detector_params),str(diff_err),str(time_err),str(whistler),str(whistler_params),str(segmented)])\n",
    "        file_name += '.result'\n",
    "        path = os.path.join(self.__result_location,self.get_site(),file_name)\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception('%s does not exists.'%path)\n",
    "        return pickle.load( open(path, \"rb\"))\n",
    "\n",
    "######### MACHINE LEARNING\n",
    "    \n",
    "    def ml_cnn_results(self, train, transforms, transforms_params, input_shape, scaler,model,save=True):\n",
    "        \"\"\"Generate all probability output of the cnn on the spectrograms\"\"\"\n",
    "        # get files from either train or test\n",
    "        files = self.get_train() if train else self.get_test()\n",
    "        samples = np.array([SampleDetector(self.__dataset_location, self.__site, file) for file in files])#[int(len(files)*0.297):int(len(files)*0.299)]\n",
    "        assert len(samples)>0, 'No samples'\n",
    "        # create multiprocessing methods\n",
    "        results = []\n",
    "        for sample in tqdm.tqdm(samples):\n",
    "            result = [sample.prob_output_ml(transforms, transforms_params, input_shape, scaler, model)]\n",
    "            results.append(result)\n",
    "        if save:\n",
    "            self.save_prob_output_ml(train, transforms, transforms_params, input_shape, results)\n",
    "        return results\n",
    "    \n",
    "    def ml_cnn_metric(self, train, transforms, transforms_params, input_shape, diff_err, time_err, window, threshold, e):\n",
    "        data = self.load_prob_ouput_ml(train, transforms, transforms_params, input_shape)\n",
    "        cnn_results = data['results']\n",
    "        metric_results = []\n",
    "        for cnn_result in tqdm.tqdm(cnn_results):\n",
    "            file, prob, shift = cnn_result[0]\n",
    "            sample = SampleDetector(self.get_dataset_location(), self.get_site(), file)\n",
    "            result = sample.evaluate_detector_ml_cnn_ouput(prob, shift ,diff_err, time_err, window,threshold, e)\n",
    "            metric_results.append(result)\n",
    "        return np.array(metric_results)\n",
    "            \n",
    "    def load_prob_ouput_ml(self,train, transforms, transforms_params, input_shape):\n",
    "        file_name = '_'.join([str(train),str(transforms),str(transforms_params),str(input_shape)])\n",
    "        file_name += '.cnn_output'\n",
    "        path = os.path.join(self.__result_location,self.get_site(),file_name)\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception('%s does not exists.'%path)\n",
    "        return pickle.load( open(path, \"rb\"))\n",
    "    \n",
    "    def save_prob_output_ml(self, train, transforms, transforms_params, input_shape, results):\n",
    "        #create parameters dictionary\n",
    "        data = {\n",
    "            'transforms': transforms,\n",
    "            'transforms_params': transforms_params, \n",
    "            'input_shape': detector, \n",
    "            'diff_err': diff_err, \n",
    "            'time_err': time_err,\n",
    "            'results': results\n",
    "        }\n",
    "        path = os.path.join(self.__result_location,self.get_site())\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        file_name = '_'.join([str(train),str(transforms),str(transforms_params),str(input_shape)])\n",
    "        file_name += '.cnn_output'\n",
    "        pickle.dump(data, open(os.path.join(path,file_name), 'wb'))\n",
    "    \n",
    "    def detector_metric_ml_single_proc(self, train, transforms, transforms_params, input_shape, scaler, model,\n",
    "                        diff_err, time_err,save=True):\n",
    "        # get files from either train or test\n",
    "        files = self.get_train() if train else self.get_test()\n",
    "        samples = np.array([SampleDetector(self.__dataset_location, self.__site, file) for file in files])#[int(len(files)*0.297):int(len(files)*0.299)]\n",
    "        assert len(samples)>0, 'No samples'\n",
    "        # create multiprocessing methods\n",
    "        results = []\n",
    "        for sample in tqdm.tqdm(samples):\n",
    "            result = sample.evaluate_detector_ml(transforms, transforms_params, input_shape, scaler, model, diff_err, time_err)\n",
    "            results.append(result)\n",
    "        results = np.array(results)\n",
    "        if save:\n",
    "            self.save_detector_metric_ml(train, transforms, transforms_params, input_shape, diff_err, time_err,results)\n",
    "        return results\n",
    "    \n",
    "    def save_detector_metric_ml(self, train, transforms, transforms_params, input_shape, diff_err, time_err, results):\n",
    "        #create parameters dictionary\n",
    "        data = {\n",
    "            'transforms': transforms,\n",
    "            'transforms_params': transforms_params, \n",
    "            'input_shape': detector, \n",
    "            'diff_err': diff_err, \n",
    "            'time_err': time_err,\n",
    "            'results': results\n",
    "        }\n",
    "        path = os.path.join(self.__result_location,self.get_site())\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        file_name = '_'.join([str(train),str(transforms),str(transforms_params),str(input_shape),\n",
    "                    str(diff_err),str(time_err)])\n",
    "        file_name += '.result'\n",
    "        pickle.dump(data, open(os.path.join(path,file_name), 'wb'))\n",
    "    \n",
    "    def load_detector_metric_ml(self,train, transforms, transforms_params, input_shape, diff_err, time_err):\n",
    "        file_name = '_'.join([str(train),str(transforms),str(transforms_params),str(input_shape),\n",
    "                              str(diff_err),str(time_err)])\n",
    "        file_name += '.result'\n",
    "        path = os.path.join(self.__result_location,self.get_site(),file_name)\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception('%s does not exists.'%path)\n",
    "        return pickle.load( open(path, \"rb\"))\n",
    "\n",
    "    \n",
    "####\n",
    "    def get_sample_per_metric_categories(results):\n",
    "        metrics = np.array([r for r in results[:,2]])\n",
    "        samples = np.array([r for r in results[:,0]])\n",
    "        most_fp, most_fn, trash = [], [], []\n",
    "        for sample, metric in zip(samples, metrics):\n",
    "            most_fp.append(sample) if metric[2]>metric[0] else most_fn.append(sample)\n",
    "            if metric[0]>metric[1] or metric[2]>metric[1]:\n",
    "                trash.append(sample)\n",
    "    \n",
    "    def load_all_detector_metric(self):\n",
    "        files = glob.glob(os.path.join(self.__result_location,self.get_site(),'*.result'))\n",
    "        metrics = []\n",
    "        for file in files:\n",
    "            results = pickle.load( open(file, \"rb\"))['results']\n",
    "            metrics.append([file,self.results(results),self.performance(results)])\n",
    "        return np.array(metrics)\n",
    "        \n",
    "    def results(self, results):\n",
    "        results = np.array([r for r in results[:,2]])\n",
    "        return np.array([results[:,0].sum(),results[:,1].sum(),results[:,2].sum()])\n",
    "    \n",
    "    def performance(self, results):\n",
    "        results = np.array([r for r in results[:,2]])\n",
    "        false_negative, true_positive, false_positive = results[:,0].sum(),results[:,1].sum(),results[:,2].sum()\n",
    "        precision = true_positive/(true_positive + false_positive)\n",
    "        recall = true_positive/(true_positive + false_negative)\n",
    "        f1_score = 2*(precision*recall/(precision+recall))\n",
    "        g_measure = sqrt((true_positive/(true_positive+false_positive))*(true_positive/(true_positive+false_negative)))\n",
    "        result = {\n",
    "            'recall': np.round(recall,3),\n",
    "            'precision': np.round(precision,3),\n",
    "            'f1 score': np.round(f1_score,3),\n",
    "            'g measure': np.round(g_measure,3),\n",
    "            'false alarm': np.round(1-precision,3),\n",
    "            'misdetection': np.round(1-recall,3)\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','datasets', 'awdEvents1')\n",
    "database_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','databases', 'awdEvents1')\n",
    "result_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','results', 'awdEvents1')\n",
    "site = 'marion'\n",
    "my_detector = Detector(dataset_loc, database_loc, result_loc, site)\n",
    "my_sample = SampleDetector(dataset_loc, site, np.random.choice(my_detector.get_train(),1)[0])\n",
    "train=True \n",
    "\n",
    "# MF\n",
    "freq_slice = [2.5,10]\n",
    "transforms=['slice','zscore']\n",
    "transforms_params=[freq_slice,[None]]\n",
    "diff_err=0.2\n",
    "time_err=0.2\n",
    "detector='tm_cfar'\n",
    "detector_params=[10,25,1e-6,10]\n",
    "whistler='sim'\n",
    "whistler_params=[3,0.7,7.5,2.5,1,25,freq_slice]\n",
    "segmented=False\n",
    "\n",
    "# my_detector.detector_metric(train, transforms, transforms_params, detector, detector_params, diff_err, \n",
    "#                             time_err, whistler, whistler_params, segmented, save=True)\n",
    "\n",
    "# ML\n",
    "aug=True\n",
    "trans, trans_params = ['zscore'],[[None]]\n",
    "params = str(trans)+'_'+str(trans_params)\n",
    "params +='_aug' if aug else ''\n",
    "scaler_path = os.path.join(database_loc,site,'models',params+'_scaler.pickle')\n",
    "model_path = os.path.join(database_loc,site,'models',params+'_model.h5')\n",
    "input_shape = [48,108] # size at req_slice[2.5,10]\n",
    "scaler = pickle.load( open( scaler_path, \"rb\" ) )['scaler']\n",
    "# model = load_model(model_path)\n",
    "\n",
    "# results = my_detector.detector_metric_ml(train, transforms, transforms_params, input_shape, scaler, model, \n",
    "#                             diff_err=diff_err, time_err=time_err, save=True)\n",
    "# results = my_detector.ml_cnn_results(train, transforms, transforms_params, input_shape, scaler, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1471/1471 [02:15<00:00, 10.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# data = my_detector.load_prob_ouput_ml(train, transforms, transforms_params, input_shape)\n",
    "# cnn_results = data['results']\n",
    "results = my_detector.ml_cnn_metric(train, transforms, transforms_params, input_shape, diff_err, time_err, window=10, threshold=0.6, e=-0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 398 2679  904]\n",
      "{'recall': 0.871, 'precision': 0.748, 'f1 score': 0.805, 'g measure': 0.807, 'false alarm': 0.252, 'misdetection': 0.129}\n"
     ]
    }
   ],
   "source": [
    "print(my_detector.results(results))\n",
    "print(my_detector.performance(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 833 2244  565]\n",
      "{'recall': 0.729, 'precision': 0.799, 'f1 score': 0.762, 'g measure': 0.763, 'false alarm': 0.201, 'misdetection': 0.271}\n",
      "[ 538 2539 1103]\n",
      "{'recall': 0.825, 'precision': 0.697, 'f1 score': 0.756, 'g measure': 0.758, 'false alarm': 0.303, 'misdetection': 0.175}\n"
     ]
    }
   ],
   "source": [
    "data = my_detector.load_detector_metric(train, transforms, transforms_params, detector,\n",
    "                                        detector_params,diff_err,time_err, whistler, \n",
    "                                        whistler_params, segmented=False)\n",
    "_results = data['results']\n",
    "print(my_detector.results(_results))\n",
    "print(my_detector.performance(_results))\n",
    "\n",
    "data = my_detector.load_detector_metric_ml(train, transforms, transforms_params, input_shape, diff_err,time_err)\n",
    "_results = data['results']\n",
    "print(my_detector.results(_results))\n",
    "print(my_detector.performance(_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','datasets', 'awdEvents1')\n",
    "# database_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','databases', 'awdEvents1')\n",
    "# result_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','results', 'awdEvents1')\n",
    "# site = 'marion'\n",
    "# my_detector = Detector(dataset_loc, database_loc, result_loc, site)\n",
    "\n",
    "# freq_slice = [2.5,10]\n",
    "# train=True \n",
    "# transforms=['slice','zscore']\n",
    "# transforms_params=[freq_slice,[None]]\n",
    "# detector='tm_cfar'\n",
    "# detector_params=[10,25,1e-6,7]\n",
    "# diff_err=0.3\n",
    "# time_err=0.2\n",
    "# whistler='sim'\n",
    "# whistler_params=[3,0.7,7.5,2.5,1,25,freq_slice]\n",
    "# segmented=False\n",
    "\n",
    "# # for decay in [3,3.25,3.5,3.75,4]:\n",
    "# #     for whistler_time in [0.5,0.6,0.7,0.8,0.9]:\n",
    "# #             for N in [10,15,20,25,30]:\n",
    "# #                 for G in [10,15,20,25,30]:\n",
    "# # for pfa in [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6]:\n",
    "# # for pfa in [1e-2, 1e-3,1e-4,1e-5,1e-6]:\n",
    "# #     for C in [6,7,8,9,10]:\n",
    "# # for size in [10,25,40,50]:\n",
    "# #     whistler_params=[3,0.7,7.5,2.5,1,size,freq_slice]\n",
    "# #     my_detector.detector_metric(train, transforms, transforms_params, detector, detector_params, diff_err, \n",
    "# #                                                         time_err, whistler, whistler_params, segmented, save=True)\n",
    "            \n",
    "            \n",
    "# # for pfa in [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6]:\n",
    "# #     for C in [0,3,5]:\n",
    "\n",
    "# # for w in [0.5,0.65,0.8,1,1.5]:\n",
    "    \n",
    "# #     data = my_detector.load_detector_metric(train=True, transforms=['zscore'], transforms_params=[[None]], detector='tm_cfar', \n",
    "# #                                 detector_params=[10,15,1e-3,0], diff_err=0.3, time_err=0.2, whistler='sim', whistler_params=[w,0.8,7.5,2.5,1,90])\n",
    "# #     results = data['results']\n",
    "# #     print(my_detector.results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','datasets', 'awdEvents1')\n",
    "# database_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','databases', 'awdEvents1')\n",
    "# result_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','results', 'awdEvents1')\n",
    "# site = 'marion'\n",
    "# my_detector = Detector(dataset_loc, database_loc, result_loc, site)\n",
    "# results = my_detector.load_all_detector_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[\"/home/othniel/wdml/data/results/awdEvents1/marion/True_['slice', 'zscore']_[[2.5, 10], [None]]_tm_cfar_[10, 25, 1e-06, 10]_0.3_0.2_sim_[3, 0.7, 7.5, 2.5, 1, 25, [2.5, 10]]_False.result\",\n",
       "         array([ 783, 2294,  513]),\n",
       "         array([0.81724261, 0.74553136, 0.77974167, 0.7805639 ])]]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results\n",
    "# arr = results[:,2]\n",
    "# arr = np.array([r for r in arr])\n",
    "# ix = np.argwhere(arr[:,3]==arr[:,3].max())\n",
    "# results[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 (1543, 1534, 100) (0.9388004895960832, 0.49853753656158595, 0.651241774570155, 0.684125195710582)\n",
      "25 (936, 2141, 377) (0.8502779984114377, 0.6958076048098798, 0.7653261840929403, 0.7691748159535654)\n",
      "40 (811, 2266, 590) (0.7934173669467787, 0.7364315892102697, 0.7638631383785606, 0.7643936240234112)\n",
      "50 (768, 2309, 676) (0.773534338358459, 0.7504062398440039, 0.7617947871989442, 0.7618825330966649)\n"
     ]
    }
   ],
   "source": [
    "# for size in [10,25,40,50]:\n",
    "#     whistler_params=[3,0.7,7.5,2.5,1,size,freq_slice]\n",
    "#     data = my_detector.load_detector_metric(train, transforms, transforms_params, detector, \n",
    "#                         detector_params, diff_err, time_err, whistler, whistler_params, segmented)\n",
    "#     results = data['results']\n",
    "#     print((size),my_detector.results(results),my_detector.performance(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_gpu",
   "language": "python",
   "name": "dp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
