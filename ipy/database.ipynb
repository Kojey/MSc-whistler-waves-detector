{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATABASE CLASS\n",
    "_Represent the database create from the dataset of samples_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import tqdm\n",
    "import parmap\n",
    "import time \n",
    "import pickle\n",
    "from itertools import repeat\n",
    "from texttable import Texttable\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import imageio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import python library\n",
    "sys.path.append(os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'py'))\n",
    "\n",
    "from dataset import Dataset\n",
    "from sample import Sample\n",
    "\n",
    "\n",
    "class Database(Dataset):\n",
    "    # Attributes\n",
    "    __train, __test = None, None\n",
    "    __train_test_file = 'train_test.pickle'\n",
    "    # Initializer\n",
    "    def __init__(self, dataset_location, database_location, site):\n",
    "        self.__database_location = database_location\n",
    "#         super(Database, self).__init__(dataset_location, site)\n",
    "        super().__init__(dataset_location, site)\n",
    "\n",
    "    def get_database_location(self):\n",
    "        return self.__database_location\n",
    "    \n",
    "    def create_param_db(self, sample):\n",
    "        sample_obj = Sample(self.get_dataset_location(), self.get_site(), sample)\n",
    "        file_name = os.path.join(self.__database_location,self.get_site(), self.get_site()+'.out')\n",
    "        file = h5py.File(file_name, 'w')\n",
    "        temp = np.array([]) \n",
    "        file_dataset = file.create_dataset(file_name, temp.shape, np.float32, compression='gzip', data=temp)\n",
    "        for key, val in sample_obj.get_spectrogram_params().items():\n",
    "            file_dataset.attrs[key] = val        \n",
    "        file.close()\n",
    "    \n",
    "    def train_test_split(self ,train_size=None, test_size=None, random_state=None, shuffle=True, save=False):\n",
    "        self.__train, self.__test = train_test_split(self.get_samples(),train_size=train_size, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
    "        if save:\n",
    "            samples = {\n",
    "                'train': self.__train,\n",
    "                'test': self.__test\n",
    "            }\n",
    "            path = os.path.join(self.__database_location,self.get_site())\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError:\n",
    "                pass\n",
    "            pickle.dump(samples, open(os.path.join(path,self.__train_test_file), 'wb'))\n",
    "    \n",
    "    def train_test_load(self):\n",
    "        path = os.path.join(self.__database_location,self.get_site(), self.__train_test_file)\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception('%s does not exists.\\n Generate the train_test file first.'%path)\n",
    "        samples = pickle.load( open(path, \"rb\")) \n",
    "        self.__train = samples['train']\n",
    "        self.__test = samples['test']\n",
    "    \n",
    "    def train_test_load_random_sample(self, test=False):\n",
    "        return np.random.choice(self.get_test()) if test else np.random.choice(self.get_train())\n",
    "    \n",
    "    def get_train(self):\n",
    "        if not self.__train:\n",
    "            self.train_test_load()\n",
    "        return self.__train\n",
    "    \n",
    "    def get_test(self):\n",
    "        if not self.__test:\n",
    "            self.train_test_load()\n",
    "        return self.__test\n",
    "        \n",
    "    def create_cut_img_db(self, sample, path, zscore=False, medfilt=False, kernel=(3,3), noise=True, shift=False):\n",
    "        '''Create jpeg cut files from a single file'''\n",
    "        sample_obj = Sample(self.get_dataset_location(), self.get_site(), sample)\n",
    "        if zscore:\n",
    "            sample_obj.apply_zscore()\n",
    "        if medfilt:\n",
    "            sample_obj.apply_medfilt(kernel=kernel)\n",
    "        if shift:\n",
    "            sample_obj.aplly_shift()\n",
    "        # cuts of 1s and 10kHz\n",
    "        cuts, whistler_count, cuts_count = sample_obj.cuts(cut_time=1, cut_freq=10, threshold=0, noise=noise)\n",
    "    \n",
    "        sample_obj.to_img()\n",
    "        img = sample_obj.get_image()\n",
    "        height = img.size[1]\n",
    "        for cut, ix in zip(cuts, range(cuts_count)) :\n",
    "            file_name = os.path.join(path,\n",
    "                                        os.path.splitext(sample)[0]+'.cut_nbr:'+\"{:02d}\".format(ix+1)+'.evt:'+str(ix<whistler_count)+'.['+str(cut[0])+':'+str(cut[1])+','+str(cut[2])+':'+str(cut[3])+'].jpeg')\n",
    "            spec = img.crop(box=(cut[2],height-cut[1],cut[3],height-cut[0]))\n",
    "            plt.figure();plt.imshow(spec);plt.show();\n",
    "            spec.save(file_name)\n",
    "    \n",
    "    def create_cut_img_db_(self, args):\n",
    "        sample, zscore, medfilt, kernel, noise, path, shift = args[0], args[1], args[2], args[3], args[4], args[5], args[6]\n",
    "        self.create_cut_img_db(sample,zscore=zscore, medfilt=medfilt, kernel=kernel, path=path, shift=shift)\n",
    "        \n",
    "    def create_cuts_img_db_mp(self, verbose=True, zscore=False, medfilt=False, kernel=(3,3), noise=True, train=True, test=True, shift=False):\n",
    "        '''Parallel implementation of create_cuts_dp'''\n",
    "        self.train_test_load()\n",
    "        if train:\n",
    "            samples = self.__train\n",
    "            samples_len = len(samples)\n",
    "            path = os.path.join(self.__database_location,self.get_site(), self.get_site()+'_image', 'train')\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError:\n",
    "                pass\n",
    "            pool = mp.Pool(mp.cpu_count())\n",
    "            if verbose:\n",
    "                # wrap arguments and use create_cut_db_ instead of create_cut_db\n",
    "                samples = [[sample, zscore, medfilt, kernel, noise, path, shift] for sample in samples]\n",
    "                for _ in tqdm.tqdm(pool.imap_unordered(self.create_cut_img_db_, samples), total=len(samples)):\n",
    "                    pass\n",
    "            else:\n",
    "                pool.map_async(self.create_cut_img_db, samples, zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise, path=path, shift=shift)\n",
    "            pool.close()\n",
    "        if test:\n",
    "            samples = self.__test\n",
    "            samples_len = len(samples)\n",
    "            path = os.path.join(self.__database_location,self.get_site(), self.get_site()+'_image', 'test')\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError:\n",
    "                pass\n",
    "            pool = mp.Pool(mp.cpu_count())\n",
    "            if verbose:\n",
    "                # wrap arguments and use create_cut_db_ instead of create_cut_db\n",
    "                samples = [[sample, zscore, medfilt, kernel, noise, path, shift] for sample in samples]\n",
    "                for _ in tqdm.tqdm(pool.imap_unordered(self.create_cut_img_db_, samples), total=len(samples)):\n",
    "                    pass\n",
    "            else:\n",
    "                pool.map_async(self.create_cut_img_db, samples, zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise, path=path, shift=shift)\n",
    "            pool.close()\n",
    "            \n",
    "        # create params output file\n",
    "        self.create_param_db(samples[0])\n",
    "        \n",
    "    def create_cut_db(self, sample,path, zscore=False, medfilt=False, kernel=(3,3), noise=True, shift=False):\n",
    "        '''Create a database from a single file'''\n",
    "        sample_obj = Sample(self.get_dataset_location(), self.get_site(), sample)\n",
    "        if zscore:\n",
    "            sample_obj.apply_zscore()\n",
    "        if medfilt:\n",
    "            sample_obj.apply_medfilt(kernel=kernel)\n",
    "        if shift:\n",
    "            sample_obj.apply_shift()\n",
    "        # cuts of 1s and 10kHz\n",
    "        cuts, whistler_count, cuts_count = sample_obj.cuts(cut_time=1, cut_freq=10, threshold=0, noise=noise)\n",
    "        for cut, ix in zip(cuts, range(cuts_count)) :\n",
    "            file_name = os.path.join(path,\n",
    "                                        os.path.splitext(sample)[0]+'.cut_nbr:'+\"{:02d}\".format(ix+1)+'.evt:'+str(ix<whistler_count)+'.['+str(cut[0])+':'+str(cut[1])+','+str(cut[2])+':'+str(cut[3])+'].h5')\n",
    "            file = h5py.File(file_name, 'w')\n",
    "            spec = sample_obj.get_spectrogram()[cut[0]:cut[1],cut[2]:cut[3]]\n",
    "            file_dataset = file.create_dataset(file_name, spec.shape, np.float32, compression='gzip', data=spec)\n",
    "            file_dataset.attrs['target'] = ix<whistler_count\n",
    "            file.close()\n",
    "    \n",
    "    def create_cut_db_(self, args):\n",
    "        sample, zscore, medfilt, kernel, noise, path, shift = args[0], args[1], args[2], args[3], args[4], args[5], args[6]\n",
    "        self.create_cut_db(sample,zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise, path=path, shift=shift)\n",
    "    \n",
    "#     def create_cuts_db(self, zscore=False, medfilt=False, kernel=(3,3), noise=True):\n",
    "#         ''''''\n",
    "#         samples = self.get_samples()\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(self.__database_location,self.get_site(), self.get_site()+'_data'))\n",
    "#         except OSError:\n",
    "#             pass\n",
    "#         for sample in tqdm.tqdm(samples):\n",
    "#             self.create_cut_db(sample, zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise)\n",
    "#         # create params output file\n",
    "#         self.create_param_db(samples[0])\n",
    "\n",
    "    def create_cuts_db_mp(self, verbose=True, zscore=False, shift=False,medfilt=False, kernel=(3,3), noise=True, train=True, test=True):\n",
    "        '''Parallel implementation of create_cuts_dp'''\n",
    "        self.train_test_load()\n",
    "        if train:\n",
    "            samples = self.__train\n",
    "            samples_len = len(samples)\n",
    "            path = os.path.join(self.__database_location,self.get_site(), self.get_site()+'_h5','train')\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError:\n",
    "                pass\n",
    "            pool = mp.Pool(mp.cpu_count())\n",
    "            if verbose:\n",
    "                # wrap arguments and use create_cut_db_ instead of create_cut_db\n",
    "                samples = [[sample, zscore, medfilt, kernel, noise, path, shift] for sample in samples]\n",
    "                for _ in tqdm.tqdm(pool.imap_unordered(self.create_cut_db_, samples), total=len(samples)):\n",
    "                    pass\n",
    "            else:\n",
    "                pool.map_async(self.create_cut_db, samples, zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise, path=path, shift=shift)\n",
    "            pool.close()\n",
    "        if test:\n",
    "            samples = self.__test\n",
    "            samples_len = len(samples)\n",
    "            path = os.path.join(self.__database_location,self.get_site(), self.get_site()+'_h5','test')\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError:\n",
    "                pass\n",
    "            pool = mp.Pool(mp.cpu_count())\n",
    "            if verbose:\n",
    "                # wrap arguments and use create_cut_db_ instead of create_cut_db\n",
    "                samples = [[sample, zscore, medfilt, kernel, noise, path, shift] for sample in samples]\n",
    "                for _ in tqdm.tqdm(pool.imap_unordered(self.create_cut_db_, samples), total=len(samples)):\n",
    "                    pass\n",
    "            else:\n",
    "                pool.map_async(self.create_cut_db, samples, zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise, path=path, shift=shift)\n",
    "            pool.close()\n",
    "        # create params output file\n",
    "        self.create_param_db(samples[0])\n",
    "    \n",
    "    def load_cut_db(self, sample):\n",
    "        '''Load one cut from the database'''\n",
    "        file = h5py.File(sample, 'r+')\n",
    "        file_data = np.empty(file[sample].shape, dtype=np.uint8)\n",
    "        file[sample].read_direct(file_data)\n",
    "        cut = np.asarray(file_data)\n",
    "        target = file[sample].attrs['target']\n",
    "        file.close()\n",
    "        return cut, target\n",
    "    \n",
    "    def load_cut_img_db(self, sample):\n",
    "        '''Load one cut from the database'''\n",
    "        cut = image.imread(sample)\n",
    "        target = 0 if sample.split('.[')[0].split('evt:')[1]=='False' else 1\n",
    "        return cut, target\n",
    "    \n",
    "#     def load_cuts_db(self):\n",
    "#         ''''''\n",
    "#         try:\n",
    "#             samples = glob.glob(os.path.join(self.__database_location, self.get_site(), self.get_site()+'_data', '*.h5'))\n",
    "#         except OSError:\n",
    "#             return None, None\n",
    "#         cuts, targets = [], []\n",
    "#         for sample in tqdm.tqdm(samples):\n",
    "#             cut, target = self.load_cut_db(sample)\n",
    "#             cuts.append(cut)\n",
    "#             targets.append(target)\n",
    "#         return np.array(cuts), np.array(targets)\n",
    "\n",
    "    def load_cuts_db_mp(self, verbose=True, test=False, shuffle=True, random_state=42):\n",
    "        ''''''\n",
    "        try:\n",
    "            directory = 'test' if test else 'train'\n",
    "            samples = glob.glob(os.path.join(self.__database_location,self.get_site(), self.get_site()+'_h5', directory,  '*.h5'))\n",
    "        except OSError:\n",
    "            return None, None\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        if verbose:\n",
    "            results = []\n",
    "            for result in tqdm.tqdm(pool.imap_unordered(self.load_cut_db, samples), total=len(samples)):\n",
    "                results.append(result)\n",
    "            results = np.array(results)\n",
    "        else:\n",
    "            results = np.array(pool.map_async(self.load_cut_db, samples).get())\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        data, targets = np.array(list(results[:,0])), np.array(results[:,1])\n",
    "        if shuffle:\n",
    "            data = list(zip(data,targets))\n",
    "            np.random.seed(random_state)\n",
    "            np.random.shuffle(data)\n",
    "            data, targets = zip(*data)\n",
    "            data, targets = np.array(data), np.array(targets)\n",
    "        return data, targets\n",
    "\n",
    "\n",
    "    def load_cuts_img_db_mp(self, verbose=True, test=False, shuffle=True, random_state=42):\n",
    "        ''''''\n",
    "        try:\n",
    "            directory = 'test' if test else 'train'\n",
    "            samples = glob.glob(os.path.join(self.__database_location,self.get_site(), self.get_site()+'_image', directory,  '*.jpeg'))\n",
    "        except OSError:\n",
    "            return None, None\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        if verbose:\n",
    "            results = []\n",
    "            for result in tqdm.tqdm(pool.imap_unordered(self.load_cut_img_db, samples), total=len(samples)):\n",
    "                results.append(result)\n",
    "            results = np.array(results)\n",
    "        else:\n",
    "            results = np.array(pool.map_async(self.load_cut_img_db, samples).get())\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        data, targets = np.array(list(results[:,0])), np.array(results[:,1])\n",
    "        if shuffle:\n",
    "            data = list(zip(data,targets))\n",
    "            np.random.seed(random_state)\n",
    "            np.random.shuffle(data)\n",
    "            data, targets = zip(*data)\n",
    "            data, targets = np.array(data), np.array(targets)\n",
    "        return data, targets\n",
    "    \n",
    "    def load_cuts_params(self):\n",
    "        file_name = os.path.join(self.__database_location,self.get_site(), self.get_site()+'.out')\n",
    "        file = h5py.File(file_name, 'r+')\n",
    "        params = {}\n",
    "        for key,val in file[file_name].attrs.items():\n",
    "            params[key] = val\n",
    "        file.close()\n",
    "        return params\n",
    "    \n",
    "    def stats(self, test=False, img=False):\n",
    "        '''Database stats'''\n",
    "        cuts, targets = self.load_cuts_img_db_mp(test=test) if img else self.load_cuts_db_mp(test=test)\n",
    "        temp_cuts = []\n",
    "        temp_cuts.append([cut.flatten() for cut in cuts])\n",
    "        temp_cuts = np.array(temp_cuts).flatten()\n",
    "        counts = np.bincount(targets)\n",
    "        counts_per = np.round(np.bincount(targets)*100/len(targets),2)\n",
    "        \n",
    "        table = Texttable()\n",
    "        table.set_deco(Texttable.HEADER)\n",
    "        table.set_header_align(['l','m'])\n",
    "        table.header(['Database statistics', ''])\n",
    "        table.set_cols_align(['l','l'])\n",
    "        table.set_cols_valign(['m','m'])\n",
    "        table.add_rows([\n",
    "                ['min',temp_cuts.min()],\n",
    "                ['max',temp_cuts.max()],\n",
    "                ['mean',temp_cuts.mean()],\n",
    "                ['std',temp_cuts.std()],\n",
    "                ['noise', str(counts[0])+'['+str(counts_per[0])+'%]'],\n",
    "                ['whistler', str(counts[1])+'['+str(counts_per[1])+'%]'],\n",
    "                ['total', len(targets)]], header=False)\n",
    "        print('\\n'+ table.draw() + '\\n')\n",
    "        return cuts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/18086 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 714/18086 [00:00<00:02, 7104.86it/s]\u001b[A\n",
      "  8%|▊         | 1417/18086 [00:00<00:02, 7080.98it/s]\u001b[A\n",
      " 12%|█▏        | 2140/18086 [00:00<00:02, 7123.97it/s]\u001b[A\n",
      " 16%|█▌        | 2851/18086 [00:00<00:02, 7118.66it/s]\u001b[A\n",
      " 19%|█▉        | 3487/18086 [00:00<00:02, 6869.33it/s]\u001b[A\n",
      " 23%|██▎       | 4221/18086 [00:00<00:01, 7002.77it/s]\u001b[A\n",
      " 28%|██▊       | 5001/18086 [00:00<00:01, 7219.13it/s]\u001b[A\n",
      " 32%|███▏      | 5786/18086 [00:00<00:01, 7395.38it/s]\u001b[A\n",
      " 36%|███▌      | 6538/18086 [00:00<00:01, 7431.97it/s]\u001b[A\n",
      " 40%|████      | 7297/18086 [00:01<00:01, 7471.54it/s]\u001b[A\n",
      " 45%|████▍     | 8064/18086 [00:01<00:01, 7526.89it/s]\u001b[A\n",
      " 49%|████▊     | 8812/18086 [00:01<00:01, 7511.03it/s]\u001b[A\n",
      " 53%|█████▎    | 9586/18086 [00:01<00:01, 7568.06it/s]\u001b[A\n",
      " 57%|█████▋    | 10382/18086 [00:01<00:01, 7678.52it/s]\u001b[A\n",
      " 62%|██████▏   | 11146/18086 [00:01<00:00, 7618.71it/s]\u001b[A\n",
      " 66%|██████▌   | 11922/18086 [00:01<00:00, 7655.08it/s]\u001b[A\n",
      " 70%|███████   | 12686/18086 [00:01<00:00, 7606.70it/s]\u001b[A\n",
      " 74%|███████▍  | 13446/18086 [00:01<00:00, 7541.62it/s]\u001b[A\n",
      " 79%|███████▊  | 14207/18086 [00:01<00:00, 7561.66it/s]\u001b[A\n",
      " 83%|████████▎ | 14967/18086 [00:02<00:00, 7571.97it/s]\u001b[A\n",
      " 87%|████████▋ | 15728/18086 [00:02<00:00, 7582.79it/s]\u001b[A\n",
      " 91%|█████████▏| 16515/18086 [00:02<00:00, 7665.71it/s]\u001b[A\n",
      " 96%|█████████▌| 17282/18086 [00:02<00:00, 7663.41it/s]\u001b[A\n",
      "100%|█████████▉| 18062/18086 [00:02<00:00, 7702.37it/s]\u001b[A\n",
      "100%|██████████| 18086/18086 [00:02<00:00, 7493.49it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# from sklearn import model_selection,preprocessing\n",
    "# dataset_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','datasets', 'awdEvents1')\n",
    "# database_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','databases', 'awdEvents1')\n",
    "# site = 'marion'\n",
    "# my_database = Database(dataset_loc, database_loc, site)\n",
    "# sample = '2013-10-20UT22:27:03.40659422.marion.cut_nbr:13.evt:True.[0:128,619:826].jpeg'\n",
    "# path = os.path.join(database_loc, site,site+'_image','train',sample)\n",
    "# cuts, targets = my_database.load_cuts_img_db_mp()\n",
    "# plt.imshow(cuts[0], cmap='gray')\n",
    "# plt.show()\n",
    "# print(targets[0])\n",
    "\n",
    "# my_database.train_test_load_random_sample(test=True)\n",
    "# file = '2013-05-20UT16:13:33.90782156.marion.vr2'\n",
    "# my_sample = Sample(dataset_loc, site, file)\n",
    "# my_sample.spectrogram_plot(figsize=(15,5))\n",
    "\n",
    "# sample = my_database.get_random_sample()\n",
    "# os.makedirs(os.path.join(my_database.get_database_location(),my_database.get_site(), my_database.get_site()+'_data'))\n",
    "# my_database.create_cut_db(file)\n",
    "# my_database.create_cuts_db_mp(zscore=True, medfilt=True, kernel=(3,3), shift=True, noise=False)\n",
    "\n",
    "# my_database.train_test_split(test_size=0.33, random_state=42, save=True)\n",
    "# my_database.train_test_load()\n",
    "# print(len(my_database.get_train()), len(my_database.get_test()))\n",
    "\n",
    "# cuts, targets = my_database.load_cuts_db_mp()\n",
    "# for cut in cuts:\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cut, cmap='jet')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_gpu",
   "language": "python",
   "name": "dp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
