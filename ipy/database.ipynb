{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATABASE CLASS\n",
    "_Represent the database create from the dataset of samples_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import tqdm\n",
    "import parmap\n",
    "import time \n",
    "import pickle\n",
    "from itertools import repeat\n",
    "from texttable import Texttable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import imageio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import python library\n",
    "sys.path.append(os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'py'))\n",
    "\n",
    "from dataset import Dataset\n",
    "from sample import Sample\n",
    "\n",
    "\n",
    "class Database(Dataset):\n",
    "    # Attributes\n",
    "    __train_test_file = 'train_test.pickle'\n",
    "    # Initializer\n",
    "    def __init__(self, dataset_location, database_location, site):\n",
    "        self.__database_location = database_location\n",
    "#         super(Database, self).__init__(dataset_location, site)\n",
    "        super().__init__(dataset_location, site)\n",
    "\n",
    "    def get_database_location(self):\n",
    "        return self.__database_location\n",
    "    \n",
    "    def create_param_db(self, sample):\n",
    "        sample_obj = Sample(self.get_dataset_location(), self.get_site(), sample)\n",
    "        file_name = os.path.join(self.__database_location,self.get_site(), self.get_site()+'.out')\n",
    "        file = h5py.File(file_name, 'w')\n",
    "        temp = np.array([]) \n",
    "        file_dataset = file.create_dataset(file_name, temp.shape, np.float32, compression='gzip', data=temp)\n",
    "        for key, val in sample_obj.get_spectrogram_params().items():\n",
    "            file_dataset.attrs[key] = val        \n",
    "        file.close()\n",
    "    \n",
    "    def train_test_split(self ,train_size=None, test_size=None, random_state=None, shuffle=True, save=False):\n",
    "        self.__train, self.__test = train_test_split(self.get_samples(),train_size=train_size, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
    "        if save:\n",
    "            samples = {\n",
    "                'train': self.__train,\n",
    "                'test': self.__test\n",
    "            }\n",
    "        path = os.path.join(self.__database_location,self.get_site(), self.get_site()+'_h5')\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        pickle.dump(samples, open(self.__train_test_file, 'wb'))\n",
    "    \n",
    "    def train_test_load(self):\n",
    "        path = os.path.join(self.__database_location,self.get_site(), self.get_site()+'_h5')\n",
    "        samples = pickle.load( open( os.path.join(path, self.__train_test_file), \"rb\" ) )\n",
    "        self.__train = samples['train']\n",
    "        self.__test = samples['test']\n",
    "    \n",
    "    def get_train(self):\n",
    "        return self.__train\n",
    "    \n",
    "    def get_test(self):\n",
    "        return self.__test\n",
    "        \n",
    "    def create_cut_img_db(self, sample, zscore=False, medfilt=False, kernel=(3,3), noise=True):\n",
    "        '''Create jpeg cut files from a single file'''\n",
    "        sample_obj = Sample(self.get_dataset_location(), self.get_site(), sample)\n",
    "        if zscore:\n",
    "            sample_obj.apply_zscore()\n",
    "        if medfilt:\n",
    "            sample_obj.apply_medfilt(kernel=kernel)\n",
    "        # cuts of 1s and 10kHz\n",
    "        cuts, whistler_count, cuts_count = sample_obj.cuts(cut_time=1, cut_freq=10, threshold=0, noise=noise)\n",
    "    \n",
    "        sample_obj.to_img()\n",
    "        img = sample_obj.get_image()\n",
    "        height = img.size[1]\n",
    "        for cut, ix in zip(cuts, range(cuts_count)) :\n",
    "            file_name = os.path.join(self.__database_location,self.get_site(), self.get_site()+'_image_data',\n",
    "                                        os.path.splitext(sample)[0]+'.cut_nbr:'+\"{:02d}\".format(ix+1)+'.evt:'+str(ix<whistler_count)+'.['+str(cut[0])+':'+str(cut[1])+','+str(cut[2])+':'+str(cut[3])+'].jpeg')\n",
    "            spec = img.crop(box=(cut[2],height-cut[1],cut[3],height-cut[0]))\n",
    "            plt.figure();plt.imshow(spec);plt.show();\n",
    "            spec.save(file_name)\n",
    "    \n",
    "    def create_cut_img_db_(self, args):\n",
    "        sample, zscore, medfilt, kernel, noise = args[0], args[1], args[2], args[3], args[4]\n",
    "        self.create_cut_img_db(sample,zscore=zscore, medfilt=medfilt, kernel=kernel)\n",
    "        \n",
    "    def create_cuts_img_db_mp(self, verbose=True, zscore=False, medfilt=False, kernel=(3,3), noise=True):\n",
    "        '''Parallel implementation of create_cuts_dp'''\n",
    "        samples = self.get_samples()\n",
    "        samples_len = len(samples)\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.__database_location,self.get_site(), self.get_site()+'_image_data'))\n",
    "        except OSError:\n",
    "            pass\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        if verbose:\n",
    "            # wrap arguments and use create_cut_db_ instead of create_cut_db\n",
    "            samples = [[sample, zscore, medfilt, kernel, noise] for sample in samples]\n",
    "            for _ in tqdm.tqdm(pool.imap_unordered(self.create_cut_img_db_, samples), total=len(samples)):\n",
    "                pass\n",
    "        else:\n",
    "            pool.map_async(self.create_cut_img_db, samples, zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise)\n",
    "        pool.close()\n",
    "        # create params output file\n",
    "        self.create_param_db(samples[0])\n",
    "        \n",
    "    def create_cut_db(self, sample, zscore=False, medfilt=False, kernel=(3,3), noise=True):\n",
    "        '''Create a database from a single file'''\n",
    "        sample_obj = Sample(self.get_dataset_location(), self.get_site(), sample)\n",
    "        if zscore:\n",
    "            sample_obj.apply_zscore()\n",
    "        if medfilt:\n",
    "            sample_obj.apply_medfilt(kernel=kernel)\n",
    "        # cuts of 1s and 10kHz\n",
    "        cuts, whistler_count, cuts_count = sample_obj.cuts(cut_time=1, cut_freq=10, threshold=0, noise=noise)\n",
    "        ### for images\n",
    "        image = True\n",
    "        if image:\n",
    "            sample_obj.to_img()\n",
    "            img = sample_obj.get_image()\n",
    "            plt.figure();plt.imshow(img);plt.show();\n",
    "        for cut, ix in zip(cuts, range(cuts_count)) :\n",
    "            file_name = os.path.join(self.__database_location,self.get_site(), self.get_site()+'_data',\n",
    "                                        os.path.splitext(sample)[0]+'.cut_nbr:'+\"{:02d}\".format(ix+1)+'.evt:'+str(ix<whistler_count)+'.['+str(cut[0])+':'+str(cut[1])+','+str(cut[2])+':'+str(cut[3])+'].h5')\n",
    "            file = h5py.File(file_name, 'w')\n",
    "            if image:\n",
    "                height = img.size[1]\n",
    "                spec = img.crop(box=(cut[2],height-cut[1],cut[3],height-cut[0]))\n",
    "                plt.figure();plt.imshow(spec);plt.show();\n",
    "                file_dataset = file.create_dataset(file_name, spec.size, np.float32, compression='gzip', data=spec)\n",
    "            else:\n",
    "                spec = sample_obj.get_spectrogram()[cut[0]:cut[1],cut[2]:cut[3]]\n",
    "                file_dataset = file.create_dataset(file_name, spec.shape, np.float32, compression='gzip', data=spec)\n",
    "            file_dataset.attrs['target'] = ix<whistler_count\n",
    "            file.close()\n",
    "    \n",
    "    def create_cut_db_(self, args):\n",
    "        sample, zscore, medfilt, kernel, noise = args[0], args[1], args[2], args[3], args[4]\n",
    "        self.create_cut_db(sample,zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise)\n",
    "    \n",
    "    def create_cuts_db(self, zscore=False, medfilt=False, kernel=(3,3), noise=True):\n",
    "        ''''''\n",
    "        samples = self.get_samples()\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.__database_location,self.get_site(), self.get_site()+'_data'))\n",
    "        except OSError:\n",
    "            pass\n",
    "        for sample in tqdm.tqdm(samples):\n",
    "            self.create_cut_db(sample, zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise)\n",
    "        # create params output file\n",
    "        self.create_param_db(samples[0])\n",
    "\n",
    "    def create_cuts_db_mp(self, verbose=True, zscore=False, medfilt=False, kernel=(3,3), noise=True):\n",
    "        '''Parallel implementation of create_cuts_dp'''\n",
    "        samples = self.get_samples()\n",
    "        samples_len = len(samples)\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.__database_location,self.get_site(), self.get_site()+'_data'))\n",
    "        except OSError:\n",
    "            pass\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        if verbose:\n",
    "            # wrap arguments and use create_cut_db_ instead of create_cut_db\n",
    "            samples = [[sample, zscore, medfilt, kernel, noise] for sample in samples]\n",
    "            for _ in tqdm.tqdm(pool.imap_unordered(self.create_cut_db_, samples), total=len(samples)):\n",
    "                pass\n",
    "        else:\n",
    "            pool.map_async(self.create_cut_db, samples, zscore=zscore, medfilt=medfilt, kernel=kernel, noise=noise)\n",
    "        pool.close()\n",
    "        # create params output file\n",
    "        self.create_param_db(samples[0])\n",
    "    \n",
    "    def load_cut_db(self, sample):\n",
    "        '''Load one cut from the database'''\n",
    "        file = h5py.File(sample, 'r+')\n",
    "        file_data = np.empty(file[sample].shape, dtype=np.uint8)\n",
    "        file[sample].read_direct(file_data)\n",
    "        cut = np.asarray(file_data)\n",
    "        target = file[sample].attrs['target']\n",
    "        file.close()\n",
    "        return cut, target\n",
    "            \n",
    "    \n",
    "    def load_cuts_db(self):\n",
    "        ''''''\n",
    "        try:\n",
    "            samples = glob.glob(os.path.join(self.__database_location, self.get_site(), self.get_site()+'_data', '*.h5'))\n",
    "        except OSError:\n",
    "            return None, None\n",
    "        cuts, targets = [], []\n",
    "        for sample in tqdm.tqdm(samples):\n",
    "            cut, target = self.load_cut_db(sample)\n",
    "            cuts.append(cut)\n",
    "            targets.append(target)\n",
    "        return np.array(cuts), np.array(targets)\n",
    "\n",
    "    def load_cuts_db_mp(self, verbose=True):\n",
    "        ''''''\n",
    "        try:\n",
    "            samples = glob.glob(os.path.join(self.__database_location,self.get_site(), self.get_site()+'_data', '*.h5'))\n",
    "        except OSError:\n",
    "            return None, None\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        if verbose:\n",
    "            results = []\n",
    "            for result in tqdm.tqdm(pool.imap_unordered(self.load_cut_db, samples), total=len(samples)):\n",
    "                results.append(result)\n",
    "            results = np.array(results)\n",
    "        else:\n",
    "            results = np.array(pool.map_async(self.load_cut_db, samples).get())\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "#         shape = np.unique([X.shape for X in results[:,0]])\n",
    "        return np.array(list(results[:,0])), np.array(results[:,1], dtype=np.bool_)\n",
    "    \n",
    "    def load_cuts_params(self):\n",
    "        file_name = os.path.join(self.__database_location,self.get_site(), self.get_site()+'.out')\n",
    "        file = h5py.File(file_name, 'r+')\n",
    "        params = {}\n",
    "        for key,val in file[file_name].attrs.items():\n",
    "            params[key] = val\n",
    "        file.close()\n",
    "        return params\n",
    "    \n",
    "    def stats(self):\n",
    "        '''Database stats'''\n",
    "        cuts, targets = self.load_cuts_db_mp()\n",
    "        temp_cuts = []\n",
    "        temp_cuts.append([cut.flatten() for cut in cuts])\n",
    "        temp_cuts = np.array(temp_cuts).flatten()\n",
    "        counts = np.bincount(targets)\n",
    "        counts_per = np.round(np.bincount(targets)*100/len(targets),2)\n",
    "        \n",
    "        table = Texttable()\n",
    "        table.set_deco(Texttable.HEADER)\n",
    "        table.set_header_align(['l','m'])\n",
    "        table.header(['Database statistics', ''])\n",
    "        table.set_cols_align(['l','l'])\n",
    "        table.set_cols_valign(['m','m'])\n",
    "        table.add_rows([\n",
    "                ['min',temp_cuts.min()],\n",
    "                ['max',temp_cuts.max()],\n",
    "                ['mean',temp_cuts.mean()],\n",
    "                ['std',temp_cuts.std()],\n",
    "                ['noise', str(counts[0])+'['+str(counts_per[0])+'%]'],\n",
    "                ['whistler', str(counts[1])+'['+str(counts_per[1])+'%]'],\n",
    "                ['total', len(targets)]], header=False)\n",
    "        print('\\n'+ table.draw() + '\\n')\n",
    "        return cuts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 2173/2196 [14:50<00:10,  2.20it/s]"
     ]
    }
   ],
   "source": [
    "# from sklearn import model_selection,preprocessing\n",
    "dataset_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','datasets', 'awdEvents1')\n",
    "database_loc = os.path.join(os.getcwd().split(os.environ.get('USER'))[0],os.environ.get('USER'), 'wdml', 'data','databases', 'awdEvents1')\n",
    "site = 'marion'\n",
    "my_database = Database(dataset_loc, database_loc, site)\n",
    "\n",
    "# file = '2013-05-20UT16:13:33.90782156.marion.vr2'\n",
    "# my_sample = Sample(dataset_loc, site, file)\n",
    "# my_sample.spectrogram_plot(figsize=(15,5))\n",
    "\n",
    "# sample = my_database.get_random_sample()\n",
    "# os.makedirs(os.path.join(my_database.get_database_location(),my_database.get_site(), my_database.get_site()+'_data'))\n",
    "# my_database.create_cut_db(file)\n",
    "my_database.create_cuts_db_mp(zscore=True, medfilt=True, kernel=(9,9), noise=False)\n",
    "\n",
    "# cuts, targets = my_database.load_cuts_db_mp()\n",
    "# for cut in cuts:\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cut, cmap='jet')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_gpu",
   "language": "python",
   "name": "dp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
